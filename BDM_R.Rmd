---
title: "hd-paad"
author: "hx"
date: "2024-10-25"
output: pdf_document
---

# Convert 5s data to 1min data

```{r}
# read in data path information

## change the file_path to actual path
# source(file.path("file_path",'make_data_directories.R'))

## install and load packages
pckgs <- c("remotes","readr","tidyr","dplyr","lubridate","stringr")

sapply(pckgs, function(x) if(!require(x,character.only=TRUE,quietly=TRUE)) {
  install.packages(x)
  require(x, character.only=TRUE)
})

## Obtain file names for all files in the directory with 5-second level data
data_5_sec_ls <- list.files(data_path_5sec)

## total number of subejcts to loop over
nid <- length(data_5_sec_ls)
## extract all subject ids
eid <- str_extract(data_5_sec_ls, "[0-9]+")

## create vector of names for wide-format data
accel_mat_nms  <- paste0("accel_mat_",1:nid)
impute_mat_nms <- paste0("impute_mat_",1:nid)

## unique minutes
utimes <- sprintf("%02d:%02d", rep(0:23, each=60), rep(0:59, 24))


  ## loop over subjects
for(i in 1:nid){
  ## read in subject i's data

  accel_i <- read.table(file.path(data_path_5sec, data_5_sec_ls[i]), header=TRUE, sep=",")

  ## verify the sampling rate for the data is 5 seconds
  (interval_i <- str_replace(names(accel_i)[1], "(.*sampleRate\\.+)([0-9]+\\.[aA-zZ]+)","\\2"))
  if(interval_i != "5.seconds") next

  ## get portion of the column names which indicates start and stop dates/times
  raw_date_i <- str_extract_all(names(accel_i)[1], "\\.{3,3}([0-9]+\\.){5,5}[0-9]+")[[1]]
  ## extract start and stop dates/times
  date_i     <- str_replace_all(raw_date_i, "\\.{3,3}","") %>%
    str_replace_all("\\.","-") %>%
    str_replace_all("([0-9]{4,4}-[0-9]{2,2}-[0-9]{2,2})-([0-9]{2,2})-([0-9]{2,2})-([0-9]{2,2})","\\1 \\2:\\3:\\4")
  ## create a vector of date-times corresponding to observed data
  ## note that this sequence of dates does NOT account for DST (uses UTC)
  date_time_vec     <- seq(ymd_hms(date_i[1], tz="UTC"),ymd_hms(date_i[2], tz="UTC"), by="5 secs")
  date_time_vec_chr <- as.POSIXlt(date_time_vec, tz="UTC")

  ## separate out the date and time components of the date/time vector
  time_vec <- sprintf("%02d:%02d", hour(date_time_vec), minute(date_time_vec))
  time_vec <- factor(time_vec, levels = utimes)
  date_vec <- sprintf("%04d-%02d-%02d",year(date_time_vec), month(date_time_vec),day(date_time_vec))

  ## combine aceleration and date data
  adf_i  <- data.frame("eid"=eid[i], accel_i, time_vec, date_vec, stringsAsFactors = FALSE)
  colnames(adf_i)[2] <- "acceleration"

  adf_i <- adf_i %>%
    group_by(date_vec, time_vec) %>%
    # avoid mask summarize function
    dplyr::summarize(acceleration = mean(acceleration, na.rm = TRUE),
              imputed = sum(1 - imputed, na.rm = TRUE),
              eid = eid[1],
              .groups = "drop")


  ## transform to wide format for accelration and imputation separately using tidyr::spread
  accel_mat_i  <- adf_i %>% select(-imputed) %>% spread(time_vec,acceleration, drop=FALSE)
  impute_mat_i <- adf_i %>% select(-acceleration) %>% spread(time_vec,imputed, drop=FALSE)

  ## assign subject i's data to the corresponding names
  assign(accel_mat_nms[i], accel_mat_i)
  assign(impute_mat_nms[i], impute_mat_i)

  ## clear up the workspace a bit
  rm(accel_i, interval_i, raw_date_i, date_i,
     date_time_vec,date_time_vec_chr,time_vec,date_vec,
     adf_i,accel_mat_i, impute_mat_i)

  print(i)
  gc()

}

## combine the data
accel_mat_nms  <- ls()[grepl("accel_mat_[0-9]+", ls())]
impute_mat_nms <- ls()[grepl("impute_mat_[0-9]+", ls())]

eval(parse(text=paste0("accel_mat <- bind_rows(", paste0(accel_mat_nms, collapse=","),")")))
eval(parse(text=paste0("impute_mat <- bind_rows(", paste0(impute_mat_nms, collapse=","),")")))

## save the combined  data
write_rds(accel_mat, file = file.path(data_path_processed, "1min_accel_add.rds"))
write_rds(impute_mat, file = file.path(data_path_processed, "1min_impute_add.rds"))
```

# Merge the data

```{r}
## read in data path information
source(file.path("",'make_data_directories.R'))

setwd('~/physical activity')

## logical for whether to delete the batch job files once data merging is complete
delete_batch <- TRUE

## If packages are not already installed, first install them and then
## load all requisite packages
pckgs <- c("dplyr","data.table")
sapply(pckgs, function(x) if(!require(x,character.only=TRUE,quietly=TRUE)) {
    install.packages(x)
    require(x, character.only=TRUE)
})
rm(pckgs)

## all minute level data to be combined
accel_sep_ls <- list.files(data_path_processed)

## load all the minute level data
for(i in seq_along(accel_sep_ls)){
    load(file.path(data_path_processed, accel_sep_ls[i]))
    print(i)
}
rm(i, accel_sep_ls, data_path_out)

## combine the data
accel_mat_nms  <- ls()[grepl("accel_mat_[0-9]+", ls())]
impute_mat_nms <- ls()[grepl("impute_mat_[0-9]+", ls())]

eval(parse(text=paste0("accel_mat <- bind_rows(", paste0(accel_mat_nms, collapse=","),")")))
eval(parse(text=paste0("impute_mat <- bind_rows(", paste0(impute_mat_nms, collapse=","),")")))

rm(list=c(accel_mat_nms, impute_mat_nms))


accel_mat <-
    accel_mat %>%
    arrange(eid, date_vec)

impute_mat <-
    impute_mat %>%
    arrange(eid, date_vec)

## save the combined data
write_rds(accel_mat, path=file.path(data_path_processed, "1min_accel.rds"))
write_rds(impute_mat, path=file.path(data_path_processed,"1min_impute.rds"))

## delete individual files
if(delete_batch){
    file.remove(file.path(data_path_processed, accel_sep_ls))
}
```

# Rearrange the data

```{r}
check<-readRDS('~/processed_data_directory/1min_accel.rds')

value_counts <- table(check$eid)

eight_values <- names(value_counts)[value_counts == 8]

check<-check[check$eid %in% eight_values,]

check<-check[1:32,]

library(dplyr)
library(tidyr)

# Generate the time column names
times <- sprintf("%02d:%02d", rep(0:23, each=60), rep(0:59, times=24))

times_transformed <- c(times[601:1440], times[1:600])  # Get the times from 10:00 to 09:59

start_idx <- which(times == '10:00')
end_idx <- which(times == '09:59')

new_data <- list()

unique_eids <- unique(check$eid)

j=0
for (eid in unique_eids) {
  i1<-Sys.time()
  eid_data <- filter(check, eid == !!eid)
  for (i in 1:7) {
    start_data <- eid_data[i, (start_idx + 2):(ncol(eid_data))]
    end_data <- eid_data[i + 1, 3:(end_idx + 2)]
    new_row <- c(eid_data[i, 1:2], t(start_data), t(end_data))
    new_data <- append(new_data, list(new_row))
  }
  i2<-Sys.time()
  i3<-i2-i1
  print(i3)
  j=j+1
  print(j)
}

# Convert the list to a dataframe
new_df <- do.call("rbind", new_data)

# Optionally, set the column names again
colnames(new_df) <- c('date_vec', 'eid', times_transformed)

# 19529

saveRDS(new_df,'~/processed_data_directory/1min_accel_10.rds')

accel<-readRDS('~/processed_data_directory/1min_accel_10.rds')

list_cols <- sapply(accel, is.list)
print(names(accel)[list_cols])
accel[names(accel)[list_cols]] <- lapply(accel[names(accel)[list_cols]], as.character)
accel[names(accel)[list_cols]] <- lapply(accel[names(accel)[list_cols]], function(x) sapply(x, paste, collapse=","))
write.table(accel, "~/processed_data_directory/1min_accel_10.csv", sep=",", row.names=FALSE)


accel<-read.csv('~/processed_data_directory/1min_accel_10.csv')

accel<-na.omit(accel)

value_counts1 <- table(accel$eid)

# Find the values that donâ€™t appear exactly 7 times
seven_values1 <- names(value_counts1)[value_counts1 == 7]
non_seven_values1<-names(value_counts1)[value_counts1 != 7]

accel<-accel[accel$eid %in% seven_values1,]
#17998 subjects

write.csv(accel,'~/processed_data_directory/1min_accel_10_noNA.csv')

accel<-read.csv('~/processed_data_directory/1min_accel_10_noNA.csv')

accel<-accel[,2:ncol(accel)]

times <- sprintf("%02d:%02d", rep(0:23, each=60), rep(0:59, times=24))

times_transformed <- c(times[601:1440], times[1:600])

# Optionally, set the column names again
colnames(accel) <- c('date_vec', 'eid', times_transformed)

write.csv(accel,'~/processed_data_directory/1min_accel_10_noNA.csv')
```

# Obtain the genetic variants from SAIGE

```{r}
SPAREAD <- read.table("~/spare_ad_signif.raw", sep = "\t", header = TRUE)

rename_column <- function(name) {
    # Check if the name starts with 'X'
    if (startsWith(name, "X")) {
        parts <- unlist(strsplit(name, "[._]"))  # Split the name by period and underscore
        # Extract the chromosome number, position, and nucleotide change
        chrom_number <- substr(parts[1], 2, nchar(parts[1]))  # Remove 'X'
        position <- parts[2]
        nucleotides <- paste(parts[5], parts[4], sep = ">")  # Combine with '>'
        new_name <- paste(chrom_number, position, nucleotides, sep = ":")
        return(new_name)
    } else {
        return(name)  # Return the original name if it doesn't start with 'X'
    }
}

colnames(SPAREAD)[7:ncol(SPAREAD)] <- sapply(colnames(SPAREAD)[7:ncol(SPAREAD)], rename_column)

# 1. Filter AD_pvalue and BA_pvalue for rows where p.value is less than 5e-8
filtered_AD_pvalue <- AD_pvalue[AD_pvalue$p.value < 5e-8, ]

# 2. Extract the MarkerID for the filtered dataframes
filtered_AD_markers <- filtered_AD_pvalue$MarkerID

# 3. Filter SPAREAD and SPAREBA to keep only columns that match the filtered MarkerIDs
# Including the first 6 columns which are not part of the filtering criteria
filtered_SPAREAD <- SPAREAD[c(1:6, which(colnames(SPAREAD) %in% filtered_AD_markers))]
write.csv(filtered_SPAREAD,'~/SPARE_AD_5e-8.csv')
```

```{r}
BA <- read.csv('~/SPARE_AD_5e-8.csv')

# 38290
BA <- BA[,2:ncol(BA)]

# 7408
BA <- na.omit(BA)

# 125986/7 = 17998
pa <- read.csv('~/processed_data_directory/1min_accel_10_noNA.csv')

pa<-pa[,2:ncol(pa)]

# 20370/7=2910
filtered_pa <- pa[pa$eid %in% BA$FID, ]

spare<-read.csv('~/SPARE.csv')

spare<-spare[,2:ncol(spare)]

spare$Date <- as.Date(spare$Date)

library(dplyr)

spare_unique <- spare %>%
                 arrange(PTID, Date) %>%
                 group_by(PTID) %>%
                 slice_min(Date)

data <- spare_unique[spare_unique$PTID %in% filtered_pa$eid, ]

# Assuming you've already loaded your dataframe as 'data'
data$BAG <- data$SPARE_BA - data$Age

# Calculate z-scores for 'BAG' and 'BMI'
data$z_BAG <- (data$BAG - mean(data$BAG)) / sd(data$BAG)
data$z_BMI <- (data$BMI - mean(data$BMI)) / sd(data$BMI)

data <- data %>%
  filter((z_BAG <= 3) & (z_BMI <= 3))


# 19383/7=2769
accel_mat_pi <- filtered_pa[filtered_pa$eid %in% data$PTID,]


library(refund)
library(dplyr)

accel_cols <- paste0("MIN",1:1440)

colnames(accel_mat_pi)<- c("date","eid",accel_cols)

x1<-accel_mat_pi[,3:ncol(accel_mat_pi)]

x1[]<-lapply(x1,as.numeric)

x1 <- mutate_all(x1, function(x) log1p(x))

#########################
# Impute "missing" data #
#########################

## perform fpca on the log transformed acceleration data
## we will use this for two purposes:
##   1) to impute the (relatively small) number of "missing" data
##   2) calculate surrogate measures based on PCA
x1 <-as.matrix(x1)

fit_fpca <- refund::fpca.face(x1, knots=50)

c(cumsum(fit_fpca$evalues)/sum(fit_fpca$evalues))

sd_pcs <- apply(fit_fpca$scores,2,sd)[1:6]
inx_plot <- c(1:1440)[seq(1,1440,by=20)]

textscale <- 2
#jpeg(file.path(figure_path, "functional_principal_components.jpeg"),height=900,width=1350,quality=100)
#png("/Users/hanxiangxu/Downloads/physical activity/supplemental_material/figures/myplot.png", width=1200, height=800)
par(mfrow=c(2,3),las=1)
#mar=c(5.1,6.1,2.1,2.1)
for(i in 1:6){
    low_tmp <- fit_fpca$mu + -2*sd_pcs[i]*fit_fpca$efunctions[,i]
    high_tmp <- fit_fpca$mu + 2*sd_pcs[i]*fit_fpca$efunctions[,i]
    matplot(inx_plot,cbind(low_tmp,fit_fpca$mu,high_tmp)[inx_plot,], type=c('p',"l","p"), pch=c("-",NA,"+"),lty=c(1,1,1),
            col=c("grey","black","grey"),xaxt='n',xlab="Time of Day",ylim=c(-0.5,6),
            main=paste("PC ", i, " Percentage of variabiity ", round(100*fit_fpca$evalues[i]/sum(fit_fpca$evalues),1), "%", sep=""),
            ylab="log(1+AC)", cex.main=textscale,cex.lab=textscale, cex.axis=textscale,
            lwd=1.5*textscale,cex=1.5*textscale)
    axis(1, at=c(1,6,12,18,23)*60 +1 , labels=c("11:00","16:00","22:00","04:00","9:00"),
         cex=textscale,cex.lab=textscale, cex.axis=textscale)
}
dev.off()
rm(list=c("sd_pcs","inx_plot","high_tmp","low_tmp","i","textscale"))


## impute missing acceleration data among good days using fpca predicted values
## truncate below by 0 since ENMO is bounded below by 0 by definition
#inx_na    <- is.na(X)
#X[inx_na] <- pmax(0,exp(fit_fpca$Yhat[inx_na]) + 1)
#rm(list=c("fit_fpca", "inx_na"))

#lX       <- log(1+x1)
#########################################
# Calculate scalar features of interest #
#########################################


## TAC: Total daily acceleration
## TLAC: Total daily log(1+acceleration)
## Sed_Mins: Total daily minutes sedentary/sleep (Note that we cannot disentale sedentary and sleep minutes without first estimating sleep time)
## MVPA_Mins: Total daily minutes spent at or above 100miligs (This has been identified as a potential cutpoint for MVPA in certain populations)
## LIPA_Mins: Total daily minutes spent in between 30 and 100 miligs (Area between "sedentary" and "MVPA")
## DARE: Daytime activity ratio estiamte, proportion of total activity acculuated between 6AM and 8PM
features_mat <-
  accel_mat_pi %>%
  dplyr::select(-one_of(c(paste0("MIN",1:1440))))

X<-accel_mat_pi[,3:ncol(accel_mat_pi)]

X[]<-lapply(X,as.numeric)

lX <- mutate_all(X, function(x) log1p(x))

features_mat$TAC <- rowSums(X)
features_mat$TLAC <- rowSums(lX)
features_mat$Sed_Mins <- rowSums(X < 30)
features_mat$MVPA_Mins <- rowSums(X >= 100)
features_mat$LIPA_Mins <- rowSums(X < 100 & X >= 30)
features_mat$DARE      <- rowSums(lX[,c(1:600,1320:1440)])/rowSums(lX)


## calcualte log acceleration accumulated in each 2-hour window of the day
tlen       <- 120
nt         <- floor(1440/tlen)
# create a list of indices for binning into 2-hour windows
inx_col_ls <- split(1:1440, rep(1:nt,each=tlen))
X_2hr      <- sapply(inx_col_ls, function(x) rowSums(lX[,x]))
colnames(X_2hr) <- paste0("TLAC_",c(1:12))

features_mat <- data.frame(features_mat, X_2hr)
rm(tlen, nt, inx_col_ls, X_2hr)

library(dplyr)
library(tidyr)

d1<-as.data.frame(fit_fpca$scores)
# Create a grouping variable for every 7 rows
d1$group <- rep(1:(nrow(d1)/7), each=7)

# Compute mean and sd for each group and reshape the data
d2 <- d1 %>%
  group_by(group) %>%
  summarise_all(list(mean = mean, sd = sd)) %>%
  ungroup() %>%
  select(-group)

# Rename the columns
colnames(d2) <- gsub("_mean", "_mean_PC", colnames(d2))
colnames(d2) <- gsub("_sd", "_sd_PC", colnames(d2))

rm(list=c('d1'))

## fragmentation measures
## SBout: average length of sedentary/sleep bouts
## ABout: average length of active bouts
## SATP: sedentary/sleep to active transition probability
## ASTP: active to sendetary/sleep transition probability
aX <- X >= 30
bout_mat <- apply(aX, 1, function(x){
  mat <- rle(x)
  sed <- mat$lengths[which(mat$values == FALSE)]
  act <- mat$lengths[which(mat$values == TRUE)]

  sed.m <- ifelse(length(sed) == 0, NA, mean(sed))
  act.m <- ifelse(length(act) == 0, NA, mean(act))

  c(sed.m, act.m)
})
features_mat$SBout <- bout_mat[1,]
features_mat$ABout <- bout_mat[2,]
features_mat$SATP  <- 1/features_mat$SBout
features_mat$ASTP  <- 1/features_mat$ABout
rm(aX, bout_mat)

## Calculate variables associated with timing of PA
## M10: average log acceleration accumulated during the 10 most active hours of the day
## L5: average log acceleration accumulated during the 5 least active hours of the day
## relA: relative amplitude = (M10-L5)/(M10 + L5)
# create transpose of the log(1+acceleration)
# we work with the transpose because colSums is MUCH faster than rowSums
# even using colSums, this takes a LONG time to run
tlX <- t(lX)

# create a list of indices for binning into 10- and 5- hour moving sums
inx_10hr_ls <- lapply(1:(1440-10*60 + 1), function(x) x:(x+(10*60-1)))
inx_5hr_ls  <- lapply(1:(1440-5*60 + 1), function(x) x:(x+(5*60-1)))
# obtain the moving sums
X_10hr     <- sapply(inx_10hr_ls, function(x) colMeans(tlX[x,,drop=FALSE]))
rm(inx_10hr_ls)
X_5hr      <- sapply(inx_5hr_ls, function(x) colMeans(tlX[x,,drop=FALSE]))
rm(inx_5hr_ls)
# calculate M10 and L5
M10 <- apply(X_10hr, 1, function(x){
  inx_x <- which.max(x)
  mx    <- x[inx_x]
  tx    <- inx_x + 300
  c(mx, tx)
})
rm(X_10hr)
L5 <- apply(X_5hr, 1, function(x){
  inx_x <- which.min(x)
  mx    <- x[inx_x]
  tx    <- inx_x + 150
  c(mx, tx)
})
rm(X_5hr, tlX)

features_mat <- data.frame(features_mat,
                           "M10" = M10[1,], "M10_t" = M10[2,],
                           "L5" = L5[1,], "L5_t" = L5[2,])
features_mat$relA    <- (features_mat$M10 - features_mat$L5)/(features_mat$M10 + features_mat$L5)
rm(X_10min, M10min, tX, inx_10min_ls, M10, L5,X)

## combine data matrices
accel_mat_pi <- left_join(accel_mat_pi, features_mat, by=c("eid","date"))

## get individual averages
## note that the timing of L5 and M10 have to be handled separately
accel_vars_daily <- c("TAC","TLAC",paste0("TLAC_",1:12),
                      "Sed_Mins","LIPA_Mins", "MVPA_Mins",
                      "SBout","ABout","SATP","ASTP",
                      "DARE","M10","L5","relA", paste0("MIN",1:1440))
accel_mat_ind <-
  accel_mat_pi %>%
  dplyr::select(one_of(c("eid",accel_vars_daily))) %>%
  group_by(eid) %>%
  summarize_all(.funs=list("mean"=mean, "sd"=sd), na.rm=TRUE) %>%
  ungroup()

accel_mat_t_vars <-
  accel_mat_pi %>%
  select(eid, "M10_t","L5_t")

## a function for calcualting the average timing of variables (in this case the M10 and L5)
get_mean_sd_t <- function(tind, tmin=0, tmax=1, search_len=1000){
  tgrid <- seq(tmin, tmax, len=search_len)
  dmat  <- outer(tind, tgrid, FUN="-")
  dmat  <- pmin(dmat^2, (1440-abs(dmat))^2)
  dsum <- colSums(dmat)
  dmin <- which.min(dsum)
  c(tgrid[dmin], sqrt(dsum[dmin]/(length(tind)-1)))
}

uid <- unique(accel_mat_t_vars$eid)
nid <- length(uid)
accel_mat_t_vars_ind <- data.frame(matrix(NA_real_, ncol=5, nrow=nid))
colnames(accel_mat_t_vars_ind) <- c("eid", "M10_t_mean","M10_t_sd","L5_t_mean", "L5_t_sd")
accel_mat_t_vars_ind[,1] <- uid
for(i in 1:nid){
  df_i <- subset(accel_mat_t_vars, eid == uid[i])
  accel_mat_t_vars_ind[i,2:3] <- get_mean_sd_t(df_i$M10_t, tmax=1440, search_len=1441)
  accel_mat_t_vars_ind[i,4:5] <- get_mean_sd_t(df_i$L5_t, tmax=1440, search_len=1441)
  if(i %% 1000 == 0) print(i)
}

accel_mat_ind <-
  accel_mat_ind %>%
  left_join(accel_mat_t_vars_ind, by="eid")

accel_mat_ind<-cbind(accel_mat_ind,d2)

full<-accel_mat_ind

rm(list=c('accel_mat_ind'))

full <- full %>%
  select(-starts_with("MIN"))

spare<-read.csv('~/SPARE.csv')

spare<-spare[,2:ncol(spare)]

spare$BAG <- spare$SPARE_BA-spare$Age

spare$Date <- as.Date(spare$Date)

library(dplyr)

# 38202
spare_unique <- spare %>%
                 arrange(PTID, Date) %>%
                 group_by(PTID) %>%
                 slice_min(Date)

# Merge the 'full' dataframe with all the selected columns from 'match'
full_with_all_columns <- merge(full, spare_unique, by.x = "eid", by.y = "PTID", all.x = TRUE)

#3598
data<-full_with_all_columns

columns_to_add <- BA[, c(1,7:ncol(BA))]

# Assume 'eid' is the corresponding column in 'data' for 'FID' in 'filtered_SPAREBA'
merged_data <- merge(data, columns_to_add, by.x = "eid", by.y = "FID", all.x = TRUE)

saveRDS(merged_data,'~/processed_data_directory/full_SPARE_AD_5e-8.rds')
```

# Plot fPCA

```{r}
library(dplyr)

accel_mat_pi <- read.csv('~/1min_accel_10_noNA.csv')

accel_mat_pi <- accel_mat_pi[,2:ncol(accel_mat_pi)]

accel_cols <- paste0("MIN",1:1440)

colnames(accel_mat_pi)<- c("date","eid",accel_cols)

x1<-accel_mat_pi[,3:ncol(accel_mat_pi)]

x1[]<-lapply(x1,as.numeric)

x1 <- mutate_all(x1, function(x) log1p(x))

#########################
# Impute "missing" data #
#########################

## perform fpca on the log transformed acceleration data
## we will use this for two purposes:
##   1) to impute the (relatively small) number of "missing" data
##   2) calculate surrogate measures based on PCA
X <-as.matrix(x1)

fit_fpca <- refund::fpca.face(X, knots=50)

c(cumsum(fit_fpca$evalues)/sum(fit_fpca$evalues))

sd_pcs <- apply(fit_fpca$scores,2,sd)[1:6]
inx_plot <- c(1:1440)[seq(1,1440,by=20)]

textscale <- 2
par(mfrow=c(2,3),las=1)
for(i in 1:6){
    low_tmp <- fit_fpca$mu + -2*sd_pcs[i]*fit_fpca$efunctions[,i]
    high_tmp <- fit_fpca$mu + 2*sd_pcs[i]*fit_fpca$efunctions[,i]
    matplot(inx_plot,cbind(low_tmp,fit_fpca$mu,high_tmp)[inx_plot,], type=c('p',"l","p"), pch=c("-",NA,"+"),lty=c(1,1,1),
            col=c("grey","black","grey"),xaxt='n',xlab="Time of Day",ylim=c(-0.5,6),
            main=paste("PC ", i, " Percentage of variabiity ", round(100*fit_fpca$evalues[i]/sum(fit_fpca$evalues),1), "%", sep=""),
            ylab="log(1+AC)", cex.main=textscale,cex.lab=textscale, cex.axis=textscale,
            lwd=1.5*textscale,cex=1.5*textscale)
    axis(1, at=c(1,6,12,18,23)*60 +1 , labels=c("11:00","16:00","22:00","04:00","9:00"),
         cex=textscale,cex.lab=textscale, cex.axis=textscale)
}
```

# Collect imaging related data from istaging dataset

```{r}
spear<-read.csv('~/istaging.csv')

library(dplyr)
# Assuming your data frame is named spear

final_df <- spear %>%
  filter(Study == "UKBIOBANK") %>%
  select(PTID, Date,Age, SPARE_BA, SPARE_AD,BMI,Sex)

final_df<-na.omit(final_df)

# male=1, female=0
final_df$Sex <- ifelse(final_df$Sex == "M", 1,
                       ifelse(final_df$Sex == "F", 0, NA))

write.csv(final_df,'~/SPARE.csv')

spare<-read.csv('~/SPARE.csv')
spare<-spare[,2:ncol(spare)]

#combine UKBB data
gi<-read.xlsx('~/gv_imaging_fulldate.xlsx')

gi$imaging_date<-gi$imaging_date-25569
gi$imaging_date<-as.Date(gi$imaging_date)

colnames(gi)[1]<-'PTID'

gispare<-merge(gi,spare,by='PTID')

write.csv(gispare,'~/gispare.csv')
```

```{r}
library(dplyr)
pa <- read.csv('~/processed_data_directory/1min_accel_10_noNA.csv')
pa <- pa[,2:ncol(pa)]

pa_time<-pa[,1:2]

ran<-pa_time %>%
     group_by(eid) %>%
     filter(date_vec == min(date_vec)) %>%
     ungroup()

colnames(ran)[2]<-'PTID'

spare<-read.csv('~/SPARE.csv')

spare<-spare[,2:ncol(spare)]

i_time<-spare[,1:2]

time_full<-merge(i_time,ran,by='PTID')

colnames(time_full)[1]<-'eid'
colnames(time_full)[2]<-'imaging_date'
colnames(time_full)[3]<-'pa_date'

dd1<-readRDS('~/full_SPARE_AD_5e-8.rds')

dd<-merge(dd1,time_full,by='eid')

dd$imaging_date <- as.Date(dd$imaging_date)
dd$pa_date <- as.Date(dd$pa_date)

dd$diff <- as.numeric(difftime(dd$imaging_date,
                                            dd$pa_date,
                                            units = "days")) / 365.25
dd$age_pa <- dd$Age-dd$diff


standardize <- function(x) {
  return((x - mean(x)) / sd(x))
}

# Apply standardization to each column
dd[2:155] <- as.data.frame(lapply(dd[,2:155], function(x) {
  if(is.numeric(x[1])) return(standardize(x))
  else return(x)
}))

saveRDS(dd,'~/full_SPARE_AD_5e-8.rds')
```

# BAMA

```{r}
hh<-readRDS('~/full_SPARE_BA_5e-8.rds')
subset_df <- hh[, c("Age", "BMI", "Sex",'PC1.x','PC2.x','PC3.x','PC4.x','PC5.x','PC6.x','PC7.x','PC8.x','PC9.x','PC10.x')]
C11<-as.matrix(subset_df)
# Assuming C11 is your original matrix
vector_form <- as.vector(C11) # Convert the matrix to a vector
numeric_vector <- as.numeric(vector_form) # Convert all elements to numeric

# Reshape back into a matrix with the same dimensions as C11
numeric_matrix <- matrix(numeric_vector, nrow = nrow(C11), ncol = ncol(C11))

C11<-numeric_matrix

is_numeric_matrix <- all(apply(C11, c(1,2), is.numeric))

full <- readRDS('~/full_SPARE_AD_5e-8.rds')

full<-full[,2:ncol(full)]

# Get the column names for mediator, exposure, and outcome
# Assuming your dataframe is named df
full <- data.frame(lapply(full, function(x) as.numeric(as.character(x))))

mediators_names <- names(full)[c(2:155)]

exposures_names <- names(full)[c(161:182)]

covariate_cols1 <- names(full)[c(227, 159, 160,211:220)]

covariate_cols2 <- names(full)[c(156, 159, 160,211:220,227)]
# Set up an empty dataframe to hold the results
results <- data.frame()
for (mediator_col in mediators_names) {
  for (exposure_col in exposures_names) {
# Loop through each mediator, exposure, and outcome

      mediator_col<- mediator

      exposure_col<- exposure

      outcome_col<-'SPARE_AD'

      # Fit the mediator model

      mediator_model <- lm(as.formula(paste0(mediator_col, " ~ ", exposure_col, " + ", paste(covariate_cols1, collapse=" + "))), data = full)

      # Fit the outcome model
      outcome_model <- lm(as.formula(paste0(outcome_col, " ~ ", mediator_col, " + ", exposure_col, " + ", paste(covariate_cols2, collapse=" + "))), data = full)

      # Run the mediation analysis
      med_out <- mediate(mediator_model, outcome_model, treat = exposure_col, mediator = mediator_col, sims = 1000, boot=TRUE)

      end_time <- Sys.time()
      execution_time <- end_time - start_time
      print(execution_time)

      # Extract the results
      effect_size <- med_out$d0 # indirect effect
      p_value <- med_out$d0.p # P-value for indirect effect

      effect_size1 <- med_out$tau.coef # indirect effect
      p_value1 <- med_out$tau.p # P-value for indirect effect

      # Add the results to the dataframe
      results <- rbind(results, data.frame(
        x = exposure_col,
        m = mediator_col,
        y = outcome_col,
        ACME = effect_size,
        ACME.p = p_value,
        TE = effect_size1,
        TE.p = p_value1
      ))
}
}

# Output the results dataframe
write.csv(results, "~/BAMA_results.csv")
```

# Univariate mediation analysis

```{r}
library(mediation)

full <- readRDS('~/full_SPARE_AD_5e-8.rds')

full<-full[,2:ncol(full)]

# Get the column names for mediator, exposure, and outcome

covariate_cols <- names(full)[c(156, 159, 160)]

# Set up an empty dataframe to hold the results
results <- data.frame()

# Loop through each mediator, exposure, and outcome
# here the range of i and j should be revised based on need
for(i in c(17)){
   for (j in c(162:169)){

      mediator_col<-colnames(full)[i]

      exposure_col<-colnames(full)[j]

      outcome_col<-'SPARE_AD'

      start_time <- Sys.time()

      # Fit the mediator model

      mediator_model <- lm(as.formula(paste0(mediator_col, " ~ ", exposure_col, " + ", paste(covariate_cols, collapse=" + "))), data = full)

      # Fit the outcome model
      outcome_model <- lm(as.formula(paste0(outcome_col, " ~ ", mediator_col, " + ", exposure_col, " + ", paste(covariate_cols, collapse=" + "))), data = full)

      # Run the mediation analysis
      med_out <- mediate(mediator_model, outcome_model, treat = exposure_col, mediator = mediator_col, sims = 1000,boot=TRUE)

      # Extract the results
      effect_size <- med_out$d0 # indirect effect
      p_value <- med_out$d0.p # P-value for indirect effect

      effect_size1 <- med_out$tau.coef # indirect effect
      p_value1 <- med_out$tau.p # P-value for indirect effect

      # Add the results to the dataframe
      results <- rbind(results, data.frame(
        x = exposure_col,
        m = mediator_col,
        y = outcome_col,
        ACME = effect_size,
        ACME.p = p_value,
        TE = effect_size1,
        TE.p = p_value1
      ))
      end_time <- Sys.time()
      execution_time <- end_time - start_time
      print(execution_time)
}
}

# Output the results dataframe
write.csv(results, "~/mediation_MVPA.csv")
```

# Plot SPARE-AD index distribution

```{r}
library(ggplot2)

j <- readRDS("C:/BDM/full_SPARE_AD_5e-8.rds")
x <- j$SPARE_AD

# Create a data frame for ggplot
data <- data.frame(SPARE_AD = x)

# Calculate percentages
positive_percentage <- sum(x > 0) / length(x) * 100
negative_percentage <- sum(x <= 0) / length(x) * 100

# Plot using ggplot2 with title alignment
ggplot(data, aes(x = SPARE_AD)) +
  geom_histogram(bins = 1000, fill = "gray", color = "black") +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed") +
  labs(title = 'The distribution of SPARE-AD index',
       x = 'SPARE_AD index', 
       y = 'Frequency') +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),  # hjust = 0.9 moves the title to the right
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 12)) +
  annotate("text", x = max(data$SPARE_AD) * 0.75, y = Inf, 
           label = sprintf("Positive: %.2f%%", positive_percentage),
           hjust = 0.7, vjust = 1.5, size = 4, color = "blue") +
  annotate("text", x = min(data$SPARE_AD) * 0.75, y = Inf, 
           label = sprintf("Negative: %.3f%%", negative_percentage),
           hjust = 0, vjust = 1.5, size = 4, color = "red") +
  scale_y_continuous(labels = scales::comma) # Format y-axis label for readability
```


# Plot the time difference of PA v.s. imaging

```{r}
# Load necessary libraries
library(ggplot2)

d <- read.csv('C:/BDM/gispare.csv')

# Assuming dates are in format "YYYY-MM-DD"
d$imaging_date <- as.Date(d$imaging_date, format = "%Y-%m-%d")
d$pa_date <- as.Date(d$pa_date, format = "%Y-%m-%d")

# Assuming 'd' is your data frame with columns 'imaging_date' and 'pa_date'
# First, calculate the difference in days
d$date_diff <- as.numeric(d$imaging_date - d$pa_date)

# Calculate percentages
total <- nrow(d)
neg_perc <- mean(d$date_diff < 0) * 100
pos_perc <- mean(d$date_diff > 0) * 100

# Plotting
p <- ggplot(d, aes(x = date_diff)) +
  stat_ecdf(geom = "step", pad = FALSE) +
  theme_minimal() +
  labs(title = "CDF of (Imaging Date - PA Date)",
       x = "Days Difference (Imaging Date - PA Date)",
       y = "Cumulative Probability") +
  scale_x_continuous(labels = function(x) format(x, big.mark = ",")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  # Add text for negative percentages
  geom_text(data = data.frame(x = min(d$date_diff), y = 1, label = sprintf("%.1f%% Negative", neg_perc)),
            aes(x = x, y = y, label = label), hjust = 0, vjust = 6) +
  # Add text for positive percentages
  geom_text(data = data.frame(x = max(d$date_diff), y = 1, label = sprintf("%.1f%% Positive", pos_perc)),
            aes(x = x, y = y, label = label), hjust = 1, vjust = 6)

# Print the plot
print(p)
```

# figure for summary statistics

```{r}
# Load necessary libraries
library(dplyr)

# Read the CSV file
data <- readRDS("/Users/hanxiangxu/Downloads/physical activity/result_GWAS+PC+timegap/full_SPARE_AD_5e-8.rds")

# Split the subjects based on SPARE-AD index being positive or negative
positive_group <- data %>% filter(SPARE_AD > 0)
negative_group <- data %>% filter(SPARE_AD <= 0)

# Count the number of subjects in both groups
count_positive <- nrow(positive_group)
count_negative <- nrow(negative_group)

# Calculate the median and range of the AGE column for both groups
age_positive_median <- median(positive_group$Age, na.rm = TRUE)
age_positive_range <- range(positive_group$Age, na.rm = TRUE)
age_negative_median <- median(negative_group$Age, na.rm = TRUE)
age_negative_range <- range(negative_group$Age, na.rm = TRUE)

# Calculate the percentage of the SEX column for both groups
# Calculate the percentage and count of the SEX column for both groups
sex_positive_count <- table(positive_group$Sex)
sex_negative_count <- table(negative_group$Sex)
sex_positive_percentage <- prop.table(sex_positive_count) * 100
sex_negative_percentage <- prop.table(sex_negative_count) * 100


# Calculate the mean and standard deviation of the BMI column for both groups
bmi_positive_mean <- mean(positive_group$BMI, na.rm = TRUE)
bmi_positive_sd <- sd(positive_group$BMI, na.rm = TRUE)
bmi_negative_mean <- mean(negative_group$BMI, na.rm = TRUE)
bmi_negative_sd <- sd(negative_group$BMI, na.rm = TRUE)

# Calculate the mean and standard deviation of SPARE-AD index for both groups
spare_ad_positive_mean <- mean(positive_group$SPARE_AD, na.rm = TRUE)
spare_ad_positive_sd <- sd(positive_group$SPARE_AD, na.rm = TRUE)
spare_ad_negative_mean <- mean(negative_group$SPARE_AD, na.rm = TRUE)
spare_ad_negative_sd <- sd(negative_group$SPARE_AD, na.rm = TRUE)

# Calculate the mean and standard deviation of MVPA_Mins_sd and MVPA_Mins_mean for both groups
mvpa_mins_sd_positive_mean <- mean(positive_group$MVPA_Mins_sd, na.rm = TRUE)
mvpa_mins_sd_positive_sd <- sd(positive_group$MVPA_Mins_sd, na.rm = TRUE)
mvpa_mins_sd_negative_mean <- mean(negative_group$MVPA_Mins_sd, na.rm = TRUE)
mvpa_mins_sd_negative_sd <- sd(negative_group$MVPA_Mins_sd, na.rm = TRUE)

mvpa_mins_mean_positive_mean <- mean(positive_group$MVPA_Mins_mean, na.rm = TRUE)
mvpa_mins_mean_positive_sd <- sd(positive_group$MVPA_Mins_mean, na.rm = TRUE)
mvpa_mins_mean_negative_mean <- mean(negative_group$MVPA_Mins_mean, na.rm = TRUE)
mvpa_mins_mean_negative_sd <- sd(negative_group$MVPA_Mins_mean, na.rm = TRUE)

# Calculate the mean and standard deviation of TAC_mean and TAC_sd for both groups
tac_mean_positive_mean <- mean(positive_group$TAC_mean, na.rm = TRUE)
tac_mean_positive_sd <- sd(positive_group$TAC_mean, na.rm = TRUE)
tac_mean_negative_mean <- mean(negative_group$TAC_mean, na.rm = TRUE)
tac_mean_negative_sd <- sd(negative_group$TAC_mean, na.rm = TRUE)

tac_sd_positive_mean <- mean(positive_group$TAC_sd, na.rm = TRUE)
tac_sd_positive_sd <- sd(positive_group$TAC_sd, na.rm = TRUE)
tac_sd_negative_mean <- mean(negative_group$TAC_sd, na.rm = TRUE)
tac_sd_negative_sd <- sd(negative_group$TAC_sd, na.rm = TRUE)

# Perform statistical tests to see if there's a significant difference between the two groups
# AGE: Using Wilcoxon rank-sum test
age_test <- wilcox.test(Age ~ (SPARE_AD > 0), data = data)

# SEX: Using Chi-squared test
sex_test <- chisq.test(table(data$Sex, data$SPARE_AD > 0))

# BMI: Using t-test
bmi_test <- t.test(BMI ~ (SPARE_AD > 0), data = data, var.equal = TRUE)

# Print results
cat("Number of subjects in Positive Group:", count_positive, "\n")
cat("Number of subjects in Negative Group:", count_negative, "\n")

cat("AGE Positive Group: Median =", age_positive_median, ", Range =", age_positive_range, "\n")
cat("AGE Negative Group: Median =", age_negative_median, ", Range =", age_negative_range, "\n")

cat("SEX Positive Group: Count =", sex_positive_count, ", Percentage =", sex_positive_percentage, "\n")
cat("SEX Negative Group: Count =", sex_negative_count, ", Percentage =", sex_negative_percentage, "\n")

cat("SEX Positive Group: Percentage =", sex_positive_percentage, "\n")
cat("SEX Negative Group: Percentage =", sex_negative_percentage, "\n")

cat("BMI Positive Group: Mean =", bmi_positive_mean, ", SD =", bmi_positive_sd, "\n")
cat("BMI Negative Group: Mean =", bmi_negative_mean, ", SD =", bmi_negative_sd, "\n")

cat("SPARE-AD Positive Group: Mean =", spare_ad_positive_mean, ", SD =", spare_ad_positive_sd, "\n")
cat("SPARE-AD Negative Group: Mean =", spare_ad_negative_mean, ", SD =", spare_ad_negative_sd, "\n")

cat("MVPA_Mins_sd Positive Group: Mean =", mvpa_mins_sd_positive_mean, ", SD =", mvpa_mins_sd_positive_sd, "\n")
cat("MVPA_Mins_sd Negative Group: Mean =", mvpa_mins_sd_negative_mean, ", SD =", mvpa_mins_sd_negative_sd, "\n")

cat("MVPA_Mins_mean Positive Group: Mean =", mvpa_mins_mean_positive_mean, ", SD =", mvpa_mins_mean_positive_sd, "\n")
cat("MVPA_Mins_mean Negative Group: Mean =", mvpa_mins_mean_negative_mean, ", SD =", mvpa_mins_mean_negative_sd, "\n")

cat("TAC_mean Positive Group: Mean =", tac_mean_positive_mean, ", SD =", tac_mean_positive_sd, "\n")
cat("TAC_mean Negative Group: Mean =", tac_mean_negative_mean, ", SD =", tac_mean_negative_sd, "\n")

cat("TAC_sd Positive Group: Mean =", tac_sd_positive_mean, ", SD =", tac_sd_positive_sd, "\n")
cat("TAC_sd Negative Group: Mean =", tac_sd_negative_mean, ", SD =", tac_sd_negative_sd, "\n")

cat("AGE Test: p-value =", age_test$p.value, "\n")
cat("SEX Test: p-value =", sex_test$p.value, "\n")
cat("BMI Test: p-value =", bmi_test$p.value, "\n")

```

```{r}
sum(positive_group$Sex)
sum(negative_group$Sex)
mean(data$SPARE_AD)
sd(data$SPARE_AD)
```

```{r}
mean(positive_group$V1_sd_PC)
sd(positive_group$V1_sd_PC)
mean(negative_group$V1_sd_PC)
sd(negative_group$V1_sd_PC)
mean(data$V1_sd_PC)
sd(data$V1_sd_PC)
age_test <- wilcox.test(V1_sd_PC ~ (SPARE_AD > 0), data = data)
print(age_test$p.value)
```

```{r}
mean(positive_group$V1_mean_PC)
sd(positive_group$V1_mean_PC)
mean(negative_group$V1_mean_PC)
sd(negative_group$V1_mean_PC)
mean(data$V1_mean_PC)
sd(data$V1_mean_PC)
age_test <- wilcox.test(V1_mean_PC ~ (SPARE_AD > 0), data = data)
print(age_test$p.value)
```

```{r}
mean(data$TAC_sd)
sd(data$TAC_sd)
age_test <- wilcox.test(TAC_sd ~ (SPARE_AD > 0), data = data)
print(age_test$p.value)
```

```{r}
mean(data$TAC_mean)
sd(data$TAC_mean)
age_test <- wilcox.test(TAC_mean ~ (SPARE_AD > 0), data = data)
print(age_test$p.value)
```

```{r}
mean(data$MVPA_Mins_mean)
sd(data$MVPA_Mins_mean)
age_test <- wilcox.test(MVPA_Mins_mean ~ (SPARE_AD > 0), data = data)
print(age_test$p.value)
```

```{r}
mean(data$MVPA_Mins_sd)
sd(data$MVPA_Mins_sd)
age_test <- wilcox.test(MVPA_Mins_sd ~ (SPARE_AD > 0), data = data)
print(age_test$p.value)
```

```{r}
age_test <- wilcox.test(SPARE_AD ~ I(SPARE_AD > 0), data = data)
print(age_test$p.value)
```

```{r}
# Split the SPARE-AD index into two groups
positive_group <- data$SPARE_AD[data$SPARE_AD > 0]
negative_group <- data$SPARE_AD[data$SPARE_AD <= 0]

# Perform Kolmogorov-Smirnov test
ks_test <- ks.test(positive_group, negative_group)
print(ks_test$p.value)
```
